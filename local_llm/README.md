# ğŸ¤– Hybrid AI Agent System

A smart AI agent that automatically switches between **OpenAI API** and **local Ollama** models with real-time streaming support and seamless fallback functionality.

## âœ¨ Features

- ğŸ”„ **Automatic Fallback**: Uses OpenAI when available, falls back to Ollama when connection fails
- ğŸŒŠ **Real-time Streaming**: Both models stream responses character by character
- ğŸ’¬ **Conversation Memory**: Maintains context across both models
- ğŸš€ **Zero Configuration**: Works automatically based on API key availability
- ğŸ›¡ï¸ **Error Handling**: Graceful fallback with informative status messages

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Ollama installed and running locally
- (Optional) OpenAI API key for enhanced performance

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd tutor_agent

# Install dependencies
uv sync

# (Optional) Set up OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
```

### Running the Agent

```bash
# Start the hybrid agent
uv run local_llm/local_llm_fallback.py
```

## ğŸ“‹ How It Works

### Step-by-Step Process

1. **ğŸ” Initialization**

   - Checks for OpenAI API key in environment variables
   - If found: Initializes OpenAI client and sets as primary model
   - If not found: Uses Ollama as primary model
   - Displays connection status with âœ… or âš ï¸ indicators

2. **ğŸ’¬ User Input**

   - Prompts user with "You: "
   - Accepts any text input
   - Supports exit commands: "exit", "quit", "bye"

3. **ğŸ§  AI Processing**

   - **OpenAI Path** (if available):
     - Sends request to OpenAI API with streaming enabled
     - Displays response character by character in real-time
     - If error occurs: Shows warning and switches to Ollama
   - **Ollama Path** (fallback or primary):
     - Sends request to local Ollama server
     - Streams response character by character
     - If streaming fails: Falls back to non-streaming mode

4. **ğŸ“ Response Handling**

   - Displays streaming response in real-time
   - Stores full response in conversation history
   - Maintains context for future interactions
   - Shows "TutorAgent: [response]" after streaming completes

5. **ğŸ”„ Continuous Loop**
   - Returns to step 2 for next user input
   - Maintains conversation context across all interactions
   - Handles model switching seamlessly during conversation

## ğŸ—ï¸ Architecture

### Core Components

- **`OllamaAgent`**: Handles local Ollama model interactions
- **`HybridAgent`**: Manages OpenAI/Ollama switching logic
- **`Agent`**: OpenAI Agents SDK integration for session management
- **`SQLiteSession`**: Persistent conversation storage

### Data Flow

```
User Input â†’ HybridAgent â†’ OpenAI (if available) â†’ Stream Response
                â†“ (if error)
            OllamaAgent â†’ Stream Response â†’ Display
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Optional: OpenAI API key for enhanced performance
OPENAI_API_KEY=your-openai-api-key

# Ollama configuration (defaults)
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.1
```

### Model Settings

- **OpenAI**: GPT-3.5-turbo with temperature 0.7
- **Ollama**: llama3.1 (configurable)
- **Streaming**: Enabled for both models
- **Fallback**: Automatic with error handling

## ğŸ“Š Status Messages

| Message                                                | Meaning                                      |
| ------------------------------------------------------ | -------------------------------------------- |
| `âœ… OpenAI API connected - using OpenAI for responses` | OpenAI is primary, Ollama is fallback        |
| `âš ï¸ OpenAI API error: [error]`                         | OpenAI failed, switching to Ollama           |
| `ğŸ”„ Switching to Ollama local model`                   | Fallback activated                           |
| `âš ï¸ Ollama streaming error: [error]`                   | Ollama streaming failed, using non-streaming |

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Ollama not running**

   ```bash
   # Start Ollama service
   ollama serve
   ```

2. **Model not found**

   ```bash
   # Pull the required model
   ollama pull llama3.1
   ```

3. **OpenAI API errors**
   - Check API key validity
   - Verify internet connection
   - System will automatically fall back to Ollama

### Error Handling

- **Connection errors**: Automatic fallback to Ollama
- **Streaming errors**: Falls back to non-streaming mode
- **Model errors**: Graceful error messages with suggestions

## ğŸ¯ Use Cases

- **Development**: Local AI development with cloud fallback
- **Privacy**: Sensitive conversations using local models
- **Reliability**: Uninterrupted service even with API failures
- **Cost-effective**: Free local processing with premium cloud backup

## ğŸ”„ Model Switching Logic

```mermaid
graph TD
    A[User Input] --> B{OpenAI Available?}
    B -->|Yes| C[Try OpenAI]
    C -->|Success| D[Stream OpenAI Response]
    C -->|Error| E[Switch to Ollama]
    B -->|No| F[Use Ollama]
    E --> G[Stream Ollama Response]
    F --> G
    G -->|Streaming Error| H[Non-streaming Fallback]
    D --> I[Store in History]
    G --> I
    H --> I
    I --> J[Display Response]
    J --> A
```

## ğŸ“ˆ Performance

- **OpenAI**: Fast, high-quality responses with streaming
- **Ollama**: Local processing, no internet required
- **Fallback**: Seamless switching without conversation loss
- **Memory**: Efficient conversation history management

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with both OpenAI and Ollama
5. Submit a pull request

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ†˜ Support

- **Issues**: Create an issue on GitHub
- **Documentation**: Check this README
- **Community**: Join our discussions

---

**Made with â¤ï¸ for the AI community**
